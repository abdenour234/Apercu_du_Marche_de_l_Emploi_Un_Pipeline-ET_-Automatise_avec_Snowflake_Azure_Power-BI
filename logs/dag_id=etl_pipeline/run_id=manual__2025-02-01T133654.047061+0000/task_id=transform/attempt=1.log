[2025-02-01T13:38:07.004+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-01T13:38:07.019+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_pipeline.transform manual__2025-02-01T13:36:54.047061+00:00 [queued]>
[2025-02-01T13:38:07.026+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_pipeline.transform manual__2025-02-01T13:36:54.047061+00:00 [queued]>
[2025-02-01T13:38:07.027+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 2
[2025-02-01T13:38:07.039+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): transform> on 2025-02-01 13:36:54.047061+00:00
[2025-02-01T13:38:07.049+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=93) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-02-01T13:38:07.051+0000] {standard_task_runner.py:72} INFO - Started process 95 to run task
[2025-02-01T13:38:07.050+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'etl_pipeline', 'transform', 'manual__2025-02-01T13:36:54.047061+00:00', '--job-id', '72', '--raw', '--subdir', 'DAGS_FOLDER/DAG.py', '--cfg-path', '/tmp/tmp5e5mmcaa']
[2025-02-01T13:38:07.052+0000] {standard_task_runner.py:105} INFO - Job 72: Subtask transform
[2025-02-01T13:38:07.093+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_pipeline.transform manual__2025-02-01T13:36:54.047061+00:00 [running]> on host 62152b012983
[2025-02-01T13:38:07.158+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_pipeline' AIRFLOW_CTX_TASK_ID='transform' AIRFLOW_CTX_EXECUTION_DATE='2025-02-01T13:36:54.047061+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-01T13:36:54.047061+00:00'
[2025-02-01T13:38:07.159+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-02-01T13:38:07.160+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-02-01T13:38:07.161+0000] {logging_mixin.py:190} INFO - Current task name:transform state:running start_date:2025-02-01 13:38:07.019982+00:00
[2025-02-01T13:38:07.162+0000] {logging_mixin.py:190} INFO - Dag name:etl_pipeline and current dag run status:running
[2025-02-01T13:38:07.162+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-02-01T13:38:07.186+0000] {warnings.py:112} WARNING - /opt/***/dags/transform.py:9: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.
  df = pd.read_json(df_json)

[2025-02-01T13:38:07.494+0000] {_universal.py:509} INFO - Request URL: 'https://myaccount123xyz.blob.core.windows.net/silver/data/data_silver.csv'
Request method: 'GET'
Request headers:
    'x-ms-range': 'REDACTED'
    'x-ms-version': 'REDACTED'
    'Accept': 'application/xml'
    'User-Agent': 'azsdk-python-storage-blob/12.24.0 Python/3.12.8 (Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.36)'
    'x-ms-date': 'REDACTED'
    'x-ms-client-request-id': 'c5386d8a-e0a1-11ef-8696-0242ac130004'
    'Authorization': 'REDACTED'
No body was attached to the request
[2025-02-01T13:38:07.800+0000] {_universal.py:545} INFO - Response status: 404
Response headers:
    'Content-Length': '215'
    'Content-Type': 'application/xml'
    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'
    'x-ms-request-id': '60982f61-d01e-0061-2fae-74f690000000'
    'x-ms-client-request-id': 'c5386d8a-e0a1-11ef-8696-0242ac130004'
    'x-ms-version': 'REDACTED'
    'x-ms-error-code': 'BlobNotFound'
    'Date': 'Sat, 01 Feb 2025 13:38:01 GMT'
[2025-02-01T13:38:07.803+0000] {logging_mixin.py:190} INFO - Blob 'data/data_silver.csv' does not exist or is empty. Creating a new one.
[2025-02-01T13:38:07.824+0000] {_universal.py:506} INFO - Request URL: 'https://myaccount123xyz.blob.core.windows.net/silver/data/data_silver.csv'
Request method: 'PUT'
Request headers:
    'Content-Length': '900821'
    'x-ms-blob-type': 'REDACTED'
    'x-ms-version': 'REDACTED'
    'Content-Type': 'application/octet-stream'
    'Accept': 'application/xml'
    'User-Agent': 'azsdk-python-storage-blob/12.24.0 Python/3.12.8 (Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.36)'
    'x-ms-date': 'REDACTED'
    'x-ms-client-request-id': 'c56ac2e4-e0a1-11ef-8696-0242ac130004'
    'Authorization': 'REDACTED'
A body is sent with the request
[2025-02-01T13:38:08.237+0000] {_universal.py:545} INFO - Response status: 201
Response headers:
    'Content-Length': '0'
    'Content-MD5': 'REDACTED'
    'Last-Modified': 'Sat, 01 Feb 2025 13:38:01 GMT'
    'ETag': '"0x8DD42C5A5F8829D"'
    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'
    'x-ms-request-id': '60983004-d01e-0061-41ae-74f690000000'
    'x-ms-client-request-id': 'c56ac2e4-e0a1-11ef-8696-0242ac130004'
    'x-ms-version': 'REDACTED'
    'x-ms-content-crc64': 'REDACTED'
    'x-ms-request-server-encrypted': 'REDACTED'
    'Date': 'Sat, 01 Feb 2025 13:38:01 GMT'
[2025-02-01T13:38:08.238+0000] {logging_mixin.py:190} INFO - Data appended successfully to blob 'data/data_silver.csv' in container 'silver'!
[2025-02-01T13:38:08.310+0000] {python.py:240} INFO - Done. Returned value was:                                              Job Title  ...            Role
0                            Machine Learning Engineer  ...  Data Scientist
1      CC - Research Scientist in Plant Bioinformatics  ...  Data Analytics
2                                 Data scientist (H/F)  ...  Data Scientist
3                   Senior Data Scientist - Casablanca  ...  Data Scientist
4    Senior Data Scientist – Machine Learning & Dee...  ...  Data Scientist
..                                                 ...  ...             ...
273  Ingénieur DevOps Site Reliability Engineer (SR...  ...  Data Analytics
274                                    DevOps Engineer  ...          DevOps
275                  Senior Cloud Architect - Data &AI  ...  Cloud Engineer
276                              Cloud Architect (F/H)  ...  Cloud Engineer
277                      DevSecOps Cloud Architect F/H  ...  Cloud Engineer

[278 rows x 14 columns]
[2025-02-01T13:38:08.352+0000] {xcom.py:690} ERROR - ("Could not convert <JobLevel.JUNIOR: 'junior'> with type JobLevel: did not recognize Python value type when inferring an Arrow data type", 'Conversion failed for column level with type object'). If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your *** config or make sure to decorate your object with attr.
[2025-02-01T13:38:08.354+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3639, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 166, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 149, in serialize
    data, serialized_classname, version, is_serialized = _serializers[qn].serialize(o)
                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serializers/pandas.py", line 49, in serialize
    table = pa.Table.from_pandas(o)
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4751, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.12/site-packages/pyarrow/pandas_compat.py", line 625, in dataframe_to_arrays
    arrays = [convert_column(c, f)
              ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pyarrow/pandas_compat.py", line 612, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.12/site-packages/pyarrow/pandas_compat.py", line 606, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 360, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 87, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: ("Could not convert <JobLevel.JUNIOR: 'junior'> with type JobLevel: did not recognize Python value type when inferring an Arrow data type", 'Conversion failed for column level with type object')
[2025-02-01T13:38:08.372+0000] {logging_mixin.py:190} INFO - Task instance in failure state
[2025-02-01T13:38:08.373+0000] {logging_mixin.py:190} INFO - Task start:2025-02-01 13:38:07.019982+00:00 end:2025-02-01 13:38:08.371913+00:00 duration:1.351931
[2025-02-01T13:38:08.373+0000] {logging_mixin.py:190} INFO - Task:<Task(PythonOperator): transform> dag:<DAG: etl_pipeline> dagrun:<DagRun etl_pipeline @ 2025-02-01 13:36:54.047061+00:00: manual__2025-02-01T13:36:54.047061+00:00, state:running, queued_at: 2025-02-01 13:36:54.063117+00:00. externally triggered: True>
[2025-02-01T13:38:08.374+0000] {logging_mixin.py:190} INFO - Failure caused by ("Could not convert <JobLevel.JUNIOR: 'junior'> with type JobLevel: did not recognize Python value type when inferring an Arrow data type", 'Conversion failed for column level with type object')
[2025-02-01T13:38:08.375+0000] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_pipeline, task_id=transform, run_id=manual__2025-02-01T13:36:54.047061+00:00, execution_date=20250201T133654, start_date=20250201T133807, end_date=20250201T133808
[2025-02-01T13:38:08.389+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-02-01T13:38:08.390+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 72 for task transform (("Could not convert <JobLevel.JUNIOR: 'junior'> with type JobLevel: did not recognize Python value type when inferring an Arrow data type", 'Conversion failed for column level with type object'); 95)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 789, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session_or_null)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3639, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py", line 166, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 249, in set
    value = cls.serialize_value(
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/xcom.py", line 688, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 105, in encode
    return super().encode(o)
           ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/json/encoder.py", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/json.py", line 92, in default
    return serialize(o)
           ^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serde.py", line 149, in serialize
    data, serialized_classname, version, is_serialized = _serializers[qn].serialize(o)
                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/serialization/serializers/pandas.py", line 49, in serialize
    table = pa.Table.from_pandas(o)
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4751, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.12/site-packages/pyarrow/pandas_compat.py", line 625, in dataframe_to_arrays
    arrays = [convert_column(c, f)
              ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pyarrow/pandas_compat.py", line 612, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.12/site-packages/pyarrow/pandas_compat.py", line 606, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 360, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 87, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: ("Could not convert <JobLevel.JUNIOR: 'junior'> with type JobLevel: did not recognize Python value type when inferring an Arrow data type", 'Conversion failed for column level with type object')
[2025-02-01T13:38:08.433+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-01T13:38:08.449+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-01T13:38:08.452+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
